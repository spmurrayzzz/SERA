base_model: Qwen3-8B
deepspeed: deepspeed_configs/zero1.json

load_in_8bit: false
load_in_4bit: false

plugins:
  - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin

chat_template: chatml
datasets:
  - path: # FILL IN
    type: chat_template
    field_messages: messages
    ds_type: json
    message_field_training: train

dataset_prepared_path: dataset_cache
output_dir: # FILL IN

sequence_len: 32768


wandb_project: 
wandb_entity:
wandb_watch:
wandb_name: 
wandb_log_model:

gradient_accumulation_steps: 8 # This can run on 4 GPUs
micro_batch_size: 1
num_epochs: 3
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 1e-5
# adamw hyperparams
adam_beta1: 0.9
adam_beta2: 0.95

bf16: auto
tf32: false

gradient_checkpointing: true
activation_offloading: true
resume_from_checkpoint:
logging_steps: 1
flash_attention: true

loss_watchdog_threshold: 5.0
loss_watchdog_patience: 3

warmup_ratio: 0.1875 # We set this to maintain 48 step warmups, so this will change with dataset. 
evals_per_epoch: 0
# save_steps: 256
save_strategy: epoch

weight_decay: 0.01
special_tokens:

# save_first_step: true  # uncomment this to validate checkpoint saving works with your config