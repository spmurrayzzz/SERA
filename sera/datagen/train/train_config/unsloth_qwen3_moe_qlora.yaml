# QLoRA configuration for Qwen3-30B-A3B (MoE) on 1-2 GPUs
#
# Memory Analysis for QLoRA:
# - Base model (4-bit): ~15GB (30B params * 0.5 bytes)
# - LoRA adapters (fp16): ~0.2GB (r=64, targeting ~1M params)
# - Optimizer (8-bit): ~0.5GB (2 bytes per LoRA param)
# - Gradients: ~0.4GB (4 bytes per LoRA param)
# - Activations (16k ctx, batch=1): ~1.5GB with gradient checkpointing
# Total: ~18GB per GPU - fits comfortably on single 24GB/40GB GPU
#
# For 2 GPUs with DDP: Each GPU holds full model, gradients synced
# Effective batch size doubles, memory per GPU stays same

# Model configuration
model:
  model_name: "Qwen/Qwen3-30B-A3B"
  max_seq_length: 32768
  load_in_4bit: true  # QLoRA - 4-bit base model
  dtype: bfloat16     # Compute in bf16

# LoRA configuration
lora:
  r: 32              # Rank - higher = more capacity, more memory
  alpha: 128         # Scaling factor (alpha/r applied to updates)
  dropout: 0.0       # Dropout disabled for better flash attention compatibility and speed
  use_rslora: false  # Rank-stabilized LoRA (experimental)
  # Target all attention and MLP projections for best quality
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Dataset configuration
dataset:
  type: jsonl
  path: # FILL IN
  split: train
  messages_field: messages
  use_chat_template: true
  text_field: text

# Training configuration
training:
  output_dir: # FILL IN

  # Batch size - start small, can increase if memory allows
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4

  # Learning rate - LoRA can use higher LR than full fine-tuning
  learning_rate: 5.0e-5
  lr_scheduler_type: cosine
  warmup_ratio: 0.1

  # Training duration
  num_train_epochs: 3
  # max_steps: 1000  # Uncomment to limit steps instead

  # Optimizer - 8-bit Adam saves ~4x optimizer memory
  optim: adamw_8bit
  weight_decay: 0.01

  # Precision
  bf16: true

  # Checkpointing
  save_strategy: epoch
  save_total_limit: 2

  # Logging
  logging_steps: 1
  gradient_checkpointing: False

# Random seed
seed: 42

# Save merged model after training (optional)
save_merged: true

# WandB logging
wandb:
  enabled: true
  project: # FILL IN
  entity: # FILL IN
  name: qwen3-moe-qlora
